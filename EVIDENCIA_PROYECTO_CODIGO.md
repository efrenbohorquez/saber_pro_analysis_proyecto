# EVIDENCIA DEL PROYECTO - AN√ÅLISIS SABER PRO
## An√°lisis Multivariante: Relaci√≥n entre Nivel Socioecon√≥mico y Rendimiento Acad√©mico

**Autor:** Proyecto de An√°lisis de Datos  
**Fecha:** Junio 2025  
**Instituci√≥n:** Universidad Nacional de Colombia  

---

## 1. DESCRIPCI√ìN DEL PROYECTO

Este proyecto realiza un an√°lisis estad√≠stico exhaustivo de la relaci√≥n entre el nivel socioecon√≥mico (NSE) y el rendimiento acad√©mico (RA) de los estudiantes que presentaron las Pruebas Saber Pro en Colombia, utilizando datos oficiales del ICFES y aplicando t√©cnicas multivariantes avanzadas.

### Objetivos del An√°lisis

1. **Describir** las caracter√≠sticas socioecon√≥micas y de rendimiento acad√©mico de la poblaci√≥n estudiantil evaluada
2. **Reducir** la dimensionalidad de las variables mediante An√°lisis de Componentes Principales (ACP) y An√°lisis Factorial (AF)
3. **Visualizar** las asociaciones entre variables socioecon√≥micas y rendimiento acad√©mico con An√°lisis de Correspondencias M√∫ltiples (ACM)
4. **Identificar** perfiles de estudiantes mediante An√°lisis de Cl√∫ster Jer√°rquico
5. **Explorar** la capacidad predictiva de las variables socioecon√≥micas sobre el rendimiento acad√©mico
6. **Analizar** la distribuci√≥n geogr√°fica de los resultados por estrato y ubicaci√≥n

---

## 2. ESTRUCTURA DEL PROYECTO

```
saber_pro_analysis_proyecto/
‚îú‚îÄ‚îÄ dashboard/                    # Dashboard interactivo en Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ app.py                   # Aplicaci√≥n principal
‚îÇ   ‚îî‚îÄ‚îÄ assets/                  # Recursos multimedia
‚îú‚îÄ‚îÄ data/                        # Datos del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ raw/                     # Datos originales
‚îÇ   ‚îî‚îÄ‚îÄ processed/               # Datos procesados
‚îú‚îÄ‚îÄ src/                         # C√≥digo fuente principal
‚îÇ   ‚îú‚îÄ‚îÄ config/                  # Configuraciones
‚îÇ   ‚îú‚îÄ‚îÄ data/                    # Carga y procesamiento de datos
‚îÇ   ‚îú‚îÄ‚îÄ models/                  # Modelos de an√°lisis
‚îÇ   ‚îî‚îÄ‚îÄ visualization/           # Visualizaciones
‚îú‚îÄ‚îÄ docs/                        # Documentaci√≥n y resultados
‚îÇ   ‚îú‚îÄ‚îÄ figures/                 # Gr√°ficos generados
‚îÇ   ‚îî‚îÄ‚îÄ reports/                 # Reportes en Word
‚îú‚îÄ‚îÄ requirements.txt             # Dependencias de Python
‚îî‚îÄ‚îÄ README.md                    # Documentaci√≥n principal
```

---

## 3. TECNOLOG√çAS UTILIZADAS

### Lenguajes y Frameworks
- **Python 3.12+**: Lenguaje principal
- **Streamlit**: Dashboard web interactivo
- **Plotly**: Visualizaciones interactivas

### Bibliotecas de An√°lisis
- **Pandas**: Manipulaci√≥n de datos
- **NumPy**: Computaci√≥n num√©rica
- **Scikit-learn**: Machine Learning y an√°lisis multivariante
- **Scipy**: An√°lisis estad√≠stico
- **Prince**: An√°lisis de Correspondencias M√∫ltiples (MCA)

### Bibliotecas de Visualizaci√≥n
- **Matplotlib**: Gr√°ficos est√°ticos
- **Seaborn**: Visualizaciones estad√≠sticas
- **Folium**: Mapas interactivos
- **GeoPandas**: An√°lisis geoespacial

---

## 4. C√ìDIGO PRINCIPAL

### 4.1 Configuraci√≥n de Constantes (`src/config/constants.py`)

```python
"""
Configuraci√≥n de constantes y rutas del proyecto.
"""
from pathlib import Path

# Rutas base del proyecto
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
DATA_DIR = PROJECT_ROOT / "data"
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"
FIGURES_DIR = PROJECT_ROOT / "docs" / "figures"
REPORTS_DIR = PROJECT_ROOT / "docs" / "reports"

# Archivos de datos
RAW_DATA_FILE = RAW_DATA_DIR / "dataset_dividido_10.csv"
PROCESSED_DATA_FILE = PROCESSED_DATA_DIR / "saber_pro_processed_data.csv"

# Variables del an√°lisis
SOCIOECONOMIC_VARS = [
    'FAMI_ESTRATOVIVIENDA',
    'FAMI_EDUCACIONPADRE',
    'FAMI_EDUCACIONMADRE',
    'FAMI_TIENEINTERNET',
    'FAMI_TIENECOMPUTADOR',
    'FAMI_TIENELAVADORA',
    'FAMI_TIENEAUTOMOVIL'
]

ACADEMIC_VARS = [
    'MOD_RAZONA_CUANTITAT_PUNT',
    'MOD_LECTURA_CRITICA_PUNT',
    'MOD_COMPETENCIAS_CIDADA_PUNT',
    'MOD_COMUNICACION_ESCRITA_PUNT',
    'MOD_INGLES_PUNT'
]

GEO_VARS = [
    'ESTU_DEPTO_RESIDE',
    'ESTU_MCPIO_RESIDE'
]

# Configuraci√≥n de colores para estratos
STRATA_COLORS = {
    'Estrato 1': '#FF4444',
    'Estrato 2': '#FF8800',
    'Estrato 3': '#FFDD00',
    'Estrato 4': '#88DD00',
    'Estrato 5': '#00DD88',
    'Estrato 6': '#0088DD'
}
```

### 4.2 An√°lisis de Componentes Principales (`src/models/pca_analysis.py`)

```python
"""
An√°lisis de Componentes Principales para variables acad√©micas.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from pathlib import Path

class PCAAnalysis:
    def __init__(self, data, academic_vars, figures_dir):
        self.data = data
        self.academic_vars = academic_vars
        self.figures_dir = Path(figures_dir)
        self.pca = None
        self.scaler = None
        self.pca_data = None
        
    def preprocess_data(self):
        """Preprocesa los datos acad√©micos para PCA."""
        # Seleccionar solo variables acad√©micas
        academic_data = self.data[self.academic_vars].copy()
        
        # Eliminar filas con valores faltantes
        academic_data = academic_data.dropna()
        
        # Estandarizar variables
        self.scaler = StandardScaler()
        academic_scaled = self.scaler.fit_transform(academic_data)
        
        return academic_scaled, academic_data.index
    
    def fit_pca(self, n_components=None):
        """Ajusta el modelo PCA."""
        academic_scaled, valid_indices = self.preprocess_data()
        
        # Ajustar PCA
        if n_components is None:
            n_components = len(self.academic_vars)
            
        self.pca = PCA(n_components=n_components)
        pca_result = self.pca.fit_transform(academic_scaled)
        
        # Crear DataFrame con resultados
        pca_columns = [f'PCA_ACADEMIC_{i+1}' for i in range(n_components)]
        self.pca_data = pd.DataFrame(
            pca_result, 
            index=valid_indices, 
            columns=pca_columns
        )
        
        # Agregar datos originales
        for col in self.data.columns:
            if col not in self.academic_vars:
                self.pca_data[col] = self.data.loc[valid_indices, col]
        
        return self.pca_data
    
    def plot_scree(self):
        """Crea gr√°fico de sedimentaci√≥n."""
        if self.pca is None:
            raise ValueError("PCA no ha sido ajustado. Ejecute fit_pca() primero.")
        
        # Calcular varianza explicada
        explained_variance = self.pca.explained_variance_ratio_
        cumulative_variance = np.cumsum(explained_variance)
        
        # Crear gr√°fico
        fig, ax = plt.subplots(figsize=(10, 6))
        
        components = range(1, len(explained_variance) + 1)
        
        ax.plot(components, explained_variance, 'bo-', label='Varianza Individual')
        ax.plot(components, cumulative_variance, 'ro-', label='Varianza Acumulada')
        ax.axhline(y=0.8, color='r', linestyle='--', label='80% Varianza')
        
        ax.set_xlabel('Componente Principal')
        ax.set_ylabel('Proporci√≥n de Varianza Explicada')
        ax.set_title('Gr√°fico de Sedimentaci√≥n - PCA Variables Acad√©micas')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Guardar gr√°fico
        plt.tight_layout()
        plt.savefig(self.figures_dir / 'pca_academic_scree.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return fig
    
    def plot_biplot(self):
        """Crea biplot del PCA."""
        if self.pca is None or self.pca_data is None:
            raise ValueError("PCA no ha sido ajustado. Ejecute fit_pca() primero.")
        
        # Obtener componentes y cargas
        pc1 = self.pca_data['PCA_ACADEMIC_1']
        pc2 = self.pca_data['PCA_ACADEMIC_2']
        loadings = self.pca.components_.T
        
        # Crear gr√°fico
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # Scatter plot de individuos
        scatter = ax.scatter(pc1, pc2, alpha=0.6, s=20)
        
        # Dibujar vectores de variables
        for i, var in enumerate(self.academic_vars):
            ax.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3, 
                    head_width=0.05, head_length=0.05, fc='red', ec='red')
            ax.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, var, 
                   fontsize=10, ha='center', va='center')
        
        ax.set_xlabel(f'PC1 ({self.pca.explained_variance_ratio_[0]:.1%} varianza)')
        ax.set_ylabel(f'PC2 ({self.pca.explained_variance_ratio_[1]:.1%} varianza)')
        ax.set_title('Biplot PCA - Variables Acad√©micas')
        ax.grid(True, alpha=0.3)
        
        # Guardar gr√°fico
        plt.tight_layout()
        plt.savefig(self.figures_dir / 'pca_academic_biplot.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return fig
    
    def plot_loadings_heatmap(self):
        """Crea mapa de calor de cargas factoriales."""
        if self.pca is None:
            raise ValueError("PCA no ha sido ajustado. Ejecute fit_pca() primero.")
        
        # Crear DataFrame de cargas
        loadings_df = pd.DataFrame(
            self.pca.components_.T,
            columns=[f'PC{i+1}' for i in range(self.pca.n_components_)],
            index=self.academic_vars
        )
        
        # Crear mapa de calor
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.heatmap(loadings_df, annot=True, cmap='RdBu_r', center=0, 
                   fmt='.3f', ax=ax)
        ax.set_title('Cargas Factoriales - PCA Variables Acad√©micas')
        
        # Guardar gr√°fico
        plt.tight_layout()
        plt.savefig(self.figures_dir / 'pca_academic_loadings.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return fig
    
    def get_component_interpretation(self):
        """Interpreta los componentes principales."""
        if self.pca is None:
            raise ValueError("PCA no ha sido ajustado. Ejecute fit_pca() primero.")
        
        loadings_df = pd.DataFrame(
            self.pca.components_.T,
            columns=[f'PC{i+1}' for i in range(self.pca.n_components_)],
            index=self.academic_vars
        )
        
        interpretations = {}
        
        for i in range(self.pca.n_components_):
            pc_name = f'PC{i+1}'
            loadings = loadings_df[pc_name].abs().sort_values(ascending=False)
            
            # Variables m√°s importantes
            top_vars = loadings.head(3).index.tolist()
            
            interpretations[pc_name] = {
                'varianza_explicada': self.pca.explained_variance_ratio_[i],
                'variables_principales': top_vars,
                'cargas': loadings_df[pc_name].to_dict()
            }
        
        return interpretations
```

### 4.3 An√°lisis de Correspondencias M√∫ltiples (`src/models/mca_analysis.py`)

```python
"""
An√°lisis de Correspondencias M√∫ltiples para variables categ√≥ricas.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from prince import MCA
from pathlib import Path

class MCAAnalysis:
    def __init__(self, data, categorical_vars, figures_dir):
        self.data = data
        self.categorical_vars = categorical_vars
        self.figures_dir = Path(figures_dir)
        self.mca = None
        self.mca_data = None
        
    def preprocess_data(self):
        """Preprocesa los datos categ√≥ricos para MCA."""
        # Seleccionar variables categ√≥ricas
        categorical_data = self.data[self.categorical_vars].copy()
        
        # Eliminar filas con valores faltantes
        categorical_data = categorical_data.dropna()
        
        # Convertir a string para asegurar tratamiento categ√≥rico
        for col in categorical_data.columns:
            categorical_data[col] = categorical_data[col].astype(str)
        
        return categorical_data
    
    def fit_mca(self, n_components=5):
        """Ajusta el modelo MCA."""
        categorical_data = self.preprocess_data()
        
        # Ajustar MCA
        self.mca = MCA(n_components=n_components, random_state=42)
        mca_result = self.mca.fit_transform(categorical_data)
        
        # Crear DataFrame con resultados
        mca_columns = [f'MCA_DIM_{i+1}' for i in range(n_components)]
        self.mca_data = pd.DataFrame(
            mca_result.values, 
            index=categorical_data.index, 
            columns=mca_columns
        )
        
        # Agregar datos originales
        for col in self.data.columns:
            if col not in self.categorical_vars:
                self.mca_data[col] = self.data.loc[categorical_data.index, col]
        
        return self.mca_data
    
    def plot_scree(self):
        """Crea gr√°fico de sedimentaci√≥n del MCA."""
        if self.mca is None:
            raise ValueError("MCA no ha sido ajustado. Ejecute fit_mca() primero.")
        
        # Obtener eigenvalues
        eigenvalues = self.mca.eigenvalues_
        explained_inertia = eigenvalues / eigenvalues.sum()
        cumulative_inertia = np.cumsum(explained_inertia)
        
        # Crear gr√°fico
        fig, ax = plt.subplots(figsize=(10, 6))
        
        dimensions = range(1, len(explained_inertia) + 1)
        
        ax.plot(dimensions, explained_inertia, 'bo-', label='Inercia Individual')
        ax.plot(dimensions, cumulative_inertia, 'ro-', label='Inercia Acumulada')
        
        ax.set_xlabel('Dimensi√≥n')
        ax.set_ylabel('Proporci√≥n de Inercia Explicada')
        ax.set_title('Gr√°fico de Sedimentaci√≥n - MCA Variables Socioecon√≥micas')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Guardar gr√°fico
        plt.tight_layout()
        plt.savefig(self.figures_dir / 'mca_socioeconomic_scree.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return fig
    
    def plot_factor_map(self):
        """Crea mapa factorial del MCA."""
        if self.mca is None:
            raise ValueError("MCA no ha sido ajustado. Ejecute fit_mca() primero.")
        
        # Obtener coordenadas de categor√≠as
        categories_coords = self.mca.column_coordinates(self.mca.X_)
        
        # Crear gr√°fico
        fig, ax = plt.subplots(figsize=(12, 10))
        
        # Plotear categor√≠as
        for i, (category, coords) in enumerate(categories_coords.iterrows()):
            ax.scatter(coords[0], coords[1], s=100, alpha=0.7)
            ax.annotate(category, (coords[0], coords[1]), 
                       xytext=(5, 5), textcoords='offset points',
                       fontsize=8, ha='left')
        
        ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)
        ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)
        ax.set_xlabel(f'Dimensi√≥n 1 ({self.mca.explained_inertia_[0]:.1%})')
        ax.set_ylabel(f'Dimensi√≥n 2 ({self.mca.explained_inertia_[1]:.1%})')
        ax.set_title('Mapa Factorial - MCA Variables Socioecon√≥micas')
        ax.grid(True, alpha=0.3)
        
        # Guardar gr√°fico
        plt.tight_layout()
        plt.savefig(self.figures_dir / 'mca_socioeconomic_factor_map.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return fig
```

### 4.4 An√°lisis de Clustering (`src/models/clustering_analysis.py`)

```python
"""
An√°lisis de clustering jer√°rquico para identificar perfiles de estudiantes.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, silhouette_samples
from scipy.cluster.hierarchy import dendrogram, linkage
from pathlib import Path

class ClusteringAnalysis:
    def __init__(self, data, cluster_vars, figures_dir):
        self.data = data
        self.cluster_vars = cluster_vars
        self.figures_dir = Path(figures_dir)
        self.scaler = None
        self.clustering = None
        self.scaled_data = None
        
    def preprocess_data(self):
        """Preprocesa los datos para clustering."""
        # Seleccionar variables de clustering
        cluster_data = self.data[self.cluster_vars].copy()
        
        # Eliminar filas con valores faltantes
        cluster_data = cluster_data.dropna()
        
        # Estandarizar variables
        self.scaler = StandardScaler()
        self.scaled_data = self.scaler.fit_transform(cluster_data)
        
        return self.scaled_data, cluster_data.index
    
    def find_optimal_clusters(self, max_clusters=10):
        """Encuentra el n√∫mero √≥ptimo de clusters usando silhouette score."""
        scaled_data, valid_indices = self.preprocess_data()
        
        silhouette_scores = []
        cluster_range = range(2, max_clusters + 1)
        
        for n_clusters in cluster_range:
            clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
            cluster_labels = clustering.fit_predict(scaled_data)
            
            # Calcular silhouette score
            score = silhouette_score(scaled_data, cluster_labels)
            silhouette_scores.append(score)
        
        # Crear gr√°fico de silhouette scores
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(cluster_range, silhouette_scores, 'bo-')
        ax.set_xlabel('N√∫mero de Clusters')
        ax.set_ylabel('Silhouette Score')
        ax.set_title('Selecci√≥n del N√∫mero √ìptimo de Clusters')
        ax.grid(True, alpha=0.3)
        
        # Marcar el mejor score
        best_n_clusters = cluster_range[np.argmax(silhouette_scores)]
        best_score = max(silhouette_scores)
        ax.scatter(best_n_clusters, best_score, color='red', s=100, zorder=5)
        ax.annotate(f'√ìptimo: {best_n_clusters} clusters\nScore: {best_score:.3f}', 
                   xy=(best_n_clusters, best_score), xytext=(10, 10),
                   textcoords='offset points', ha='left')
        
        # Guardar gr√°fico
        plt.tight_layout()
        plt.savefig(self.figures_dir / 'cluster_silhouette_scores.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return best_n_clusters, silhouette_scores
    
    def fit_clustering(self, n_clusters=None):
        """Ajusta el modelo de clustering."""
        scaled_data, valid_indices = self.preprocess_data()
        
        if n_clusters is None:
            n_clusters, _ = self.find_optimal_clusters()
        
        # Ajustar clustering
        self.clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
        cluster_labels = self.clustering.fit_predict(scaled_data)
        
        # Crear DataFrame con resultados
        cluster_data = self.data.loc[valid_indices].copy()
        cluster_data['CLUSTER'] = cluster_labels
        
        return cluster_data
    
    def plot_dendrogram(self):
        """Crea dendrograma del clustering jer√°rquico."""
        scaled_data, _ = self.preprocess_data()
        
        # Calcular linkage matrix
        linkage_matrix = linkage(scaled_data, method='ward')
        
        # Crear dendrograma
        fig, ax = plt.subplots(figsize=(15, 8))
        dendrogram(linkage_matrix, ax=ax, truncate_mode='level', p=5)
        ax.set_title('Dendrograma - Clustering Jer√°rquico')
        ax.set_xlabel('√çndice de la muestra o (tama√±o del cluster)')
        ax.set_ylabel('Distancia')
        
        # Guardar gr√°fico
        plt.tight_layout()
        plt.savefig(self.figures_dir / 'cluster_dendrogram.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return fig
    
    def plot_cluster_profiles(self, cluster_data):
        """Crea gr√°fico de perfiles de clusters."""
        # Calcular medias por cluster
        cluster_means = cluster_data.groupby('CLUSTER')[self.cluster_vars].mean()
        
        # Crear gr√°fico
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # Normalizar para mejor visualizaci√≥n
        cluster_means_norm = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min())
        
        # Plotear perfiles
        for cluster in cluster_means_norm.index:
            ax.plot(range(len(self.cluster_vars)), cluster_means_norm.loc[cluster], 
                   marker='o', linewidth=2, label=f'Cluster {cluster}')
        
        ax.set_xticks(range(len(self.cluster_vars)))
        ax.set_xticklabels(self.cluster_vars, rotation=45, ha='right')
        ax.set_ylabel('Valor Normalizado')
        ax.set_title('Perfiles de Clusters')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Guardar gr√°fico
        plt.tight_layout()
        plt.savefig(self.figures_dir / 'cluster_profiles.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return fig
```

### 4.5 Dashboard Principal (`dashboard/app.py`)

```python
"""
Aplicaci√≥n principal del dashboard en Streamlit para visualizar los resultados del an√°lisis de Saber Pro.
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
import sys
import os
from pathlib import Path

# Manejo de importaci√≥n de folium y streamlit_folium con fallback
FOLIUM_AVAILABLE = False
try:
    import folium
    from streamlit_folium import st_folium, folium_static
    FOLIUM_AVAILABLE = True
except ImportError as e:
    st.warning(f"üó∫Ô∏è Funcionalidades de mapas no disponibles: {e}")
    st.info("üí° Para habilitar mapas interactivos, instale: pip install folium streamlit-folium")
    
    # Definir funciones dummy para evitar errores
    def st_folium(*args, **kwargs):
        st.error("streamlit_folium no est√° disponible")
        return None
    
    def folium_static(*args, **kwargs):
        st.error("streamlit_folium no est√° disponible")
        return None

# Configuraci√≥n de la plantilla estilo The Economist
economist_template = go.layout.Template()
economist_template.layout = go.Layout(
    font_family="Arial, sans-serif",
    font_color="#2E2E2E",
    title_font_family="Georgia, serif",
    title_font_color="#2E2E2E",
    title_x=0.01,
    title_y=0.95,
    title_yanchor='top',
    title_xanchor='left',
    title_pad_t=20,
    title_pad_l=10,
    title_font_size=20,
    plot_bgcolor='white',
    paper_bgcolor='white',
    colorway=['#FF9999', '#757575', '#BDBDBD', '#2E2E2E', '#FFB6C1', '#87CEEB'],
    margin=dict(l=80, r=30, t=100, b=80)
)

# Registrar plantilla
pio.templates['economist'] = economist_template
pio.templates.default = 'economist'

# Agregar el directorio ra√≠z al path
sys.path.append(str(Path(__file__).resolve().parent.parent))

from src.config.constants import (
    PROCESSED_DATA_DIR,
    FIGURES_DIR,
    SOCIOECONOMIC_VARS,
    ACADEMIC_VARS,
    GEO_VARS,
    STRATA_COLORS
)

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="Dashboard An√°lisis Saber Pro",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Funciones de carga de datos
@st.cache_data
def load_data():
    """Carga los datos procesados."""
    from src.config.constants import RAW_DATA_FILE
    
    try:
        df = pd.read_csv(RAW_DATA_FILE)
        st.success(f"Datos cargados exitosamente desde {RAW_DATA_FILE}")
        return df
    except FileNotFoundError:
        st.error(f"No se encontr√≥ el archivo de datos: {RAW_DATA_FILE}")
        return None
    except Exception as e:
        st.error(f"Error al cargar los datos: {e}")
        return None

# Funci√≥n principal
def main():
    """Funci√≥n principal del dashboard."""
    show_header()
    
    # Men√∫ lateral
    st.sidebar.title("üß≠ Navegaci√≥n")
    
    # Verificar disponibilidad de folium para ajustar men√∫
    if FOLIUM_AVAILABLE:
        geospatial_option = "üìç An√°lisis Geoespacial"
    else:
        geospatial_option = "üìä An√°lisis Geoespacial (Solo Gr√°ficos)"
    
    page = st.sidebar.selectbox(
        "Seleccione una secci√≥n:",
        [
            "üè† Inicio",
            "üîç Exploraci√≥n de Datos",
            "üß© An√°lisis de Componentes Principales",
            "üîÑ An√°lisis de Correspondencias M√∫ltiples",
            "üéØ Clustering Jer√°rquico",
            "ü§ñ Modelos Predictivos",
            geospatial_option
        ]
    )
    
    # Mostrar p√°gina seleccionada
    if page == "üè† Inicio":
        show_home()
    elif page == "üîç Exploraci√≥n de Datos":
        show_eda()
    elif page == "üß© An√°lisis de Componentes Principales":
        show_pca()
    elif page == "üîÑ An√°lisis de Correspondencias M√∫ltiples":
        show_mca()
    elif page == "üéØ Clustering Jer√°rquico":
        show_clustering()
    elif page == "ü§ñ Modelos Predictivos":
        show_models()
    elif page in ["üìç An√°lisis Geoespacial", "üìä An√°lisis Geoespacial (Solo Gr√°ficos)"]:
        show_geospatial()

def show_header():
    """Muestra el encabezado del dashboard."""
    st.title("üìä Dashboard An√°lisis Saber Pro")
    st.markdown("""
    Este dashboard presenta un an√°lisis multivariado de la relaci√≥n entre el nivel socioecon√≥mico 
    y el rendimiento acad√©mico en las pruebas Saber Pro en Colombia.
    """)
    st.markdown("---")

def show_home():
    """Muestra la p√°gina de inicio del dashboard."""
    st.header("üè† Inicio")
    
    # Mostrar banner si existe
    banner_path = Path(__file__).parent / "assets" / "484798221_1031777162319148_3372633552707418771_n.jpg"
    if banner_path.exists():
        st.image(str(banner_path), use_container_width=True)
    
    st.markdown("""
    ## An√°lisis Multivariante: Relaci√≥n entre Nivel Socioecon√≥mico y Rendimiento Acad√©mico
    
    Este proyecto realiza un an√°lisis estad√≠stico exhaustivo utilizando t√©cnicas multivariantes avanzadas.
    
    ### Objetivos del An√°lisis
    
    1. **Describir** las caracter√≠sticas socioecon√≥micas y de rendimiento acad√©mico
    2. **Reducir** la dimensionalidad mediante ACP y AF
    3. **Visualizar** asociaciones con ACM
    4. **Identificar** perfiles mediante clustering
    5. **Explorar** capacidad predictiva
    6. **Analizar** distribuci√≥n geogr√°fica
    """)

if __name__ == "__main__":
    main()
```

---

## 5. RESULTADOS Y AN√ÅLISIS

### 5.1 An√°lisis Descriptivo
- **Total de estudiantes analizados**: [N√∫mero seg√∫n dataset]
- **Variables acad√©micas**: 5 m√≥dulos de competencias
- **Variables socioecon√≥micas**: 7 indicadores principales
- **Cobertura geogr√°fica**: Todos los departamentos de Colombia

### 5.2 T√©cnicas Multivariantes Aplicadas

#### An√°lisis de Componentes Principales (ACP)
- Reducci√≥n de dimensionalidad de variables acad√©micas
- Identificaci√≥n de componentes principales del rendimiento
- Interpretaci√≥n de factores subyacentes

#### An√°lisis de Correspondencias M√∫ltiples (ACM)
- Exploraci√≥n de asociaciones entre variables categ√≥ricas
- Visualizaci√≥n de perfiles socioecon√≥micos
- Mapas factoriales de categor√≠as

#### Clustering Jer√°rquico
- Identificaci√≥n de perfiles de estudiantes
- Segmentaci√≥n basada en caracter√≠sticas acad√©micas y socioecon√≥micas
- An√°lisis de dendrogramas y silhouette scores

### 5.3 Modelos Predictivos
- Regresi√≥n Log√≠stica
- Random Forest
- Comparaci√≥n de m√©tricas de rendimiento
- An√°lisis de importancia de variables

---

## 6. TECNOLOG√çAS Y HERRAMIENTAS

### Entorno de Desarrollo
- **Python 3.12+**
- **Jupyter Notebooks** para an√°lisis exploratorio
- **VS Code** como IDE principal
- **Git** para control de versiones

### Librer√≠as Principales
```python
# An√°lisis de datos
pandas==2.1.3
numpy==1.24.3
scipy==1.11.4

# Machine Learning
scikit-learn==1.3.2
prince==0.7.1

# Visualizaci√≥n
matplotlib==3.8.2
seaborn==0.13.0
plotly==5.17.0

# Dashboard
streamlit==1.28.1
streamlit-folium==0.25.0

# An√°lisis geoespacial
folium==0.15.0
geopandas==0.14.1
```

### Despliegue
- **Streamlit Cloud** para hosting del dashboard
- **GitHub** para repositorio del c√≥digo
- **Requirements.txt** para gesti√≥n de dependencias

---

## 7. ESTRUCTURA DE ARCHIVOS GENERADOS

### Datos Procesados
```
data/processed/
‚îú‚îÄ‚îÄ saber_pro_processed_data.csv      # Dataset principal procesado
‚îú‚îÄ‚îÄ pca_academic_results.csv          # Resultados del ACP
‚îú‚îÄ‚îÄ mca_socioeconomic_results.csv     # Resultados del ACM
‚îú‚îÄ‚îÄ clustering_results.csv            # Resultados del clustering
‚îú‚îÄ‚îÄ model_comparison.csv              # Comparaci√≥n de modelos
‚îú‚îÄ‚îÄ department_stats.csv              # Estad√≠sticas por departamento
‚îî‚îÄ‚îÄ geospatial_data.csv              # Datos geoespaciales
```

### Visualizaciones
```
docs/figures/
‚îú‚îÄ‚îÄ pca_academic_scree.png            # Gr√°fico de sedimentaci√≥n PCA
‚îú‚îÄ‚îÄ pca_academic_biplot.png           # Biplot PCA
‚îú‚îÄ‚îÄ pca_academic_loadings.png         # Cargas factoriales PCA
‚îú‚îÄ‚îÄ mca_socioeconomic_scree.png       # Gr√°fico de sedimentaci√≥n MCA
‚îú‚îÄ‚îÄ mca_socioeconomic_factor_map.png  # Mapa factorial MCA
‚îú‚îÄ‚îÄ cluster_dendrogram.png            # Dendrograma clustering
‚îú‚îÄ‚îÄ cluster_profiles.png              # Perfiles de clusters
‚îú‚îÄ‚îÄ model_comparison.png              # Comparaci√≥n de modelos
‚îî‚îÄ‚îÄ performance_by_department.png     # Rendimiento por departamento
```

---

## 8. CONFIGURACI√ìN DEL ENTORNO

### Instalaci√≥n de Dependencias

```bash
# Clonar repositorio
git clone https://github.com/efrenbohorquez/saber_pro_analysis_proyecto.git
cd saber_pro_analysis_proyecto

# Crear entorno virtual
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Instalar dependencias
pip install -r requirements.txt
```

### Ejecuci√≥n del An√°lisis

```bash
# Ejecutar an√°lisis completo
python src/main.py

# Ejecutar dashboard
streamlit run dashboard/app.py
```

### Estructura de Configuraci√≥n

```python
# requirements.txt
streamlit==1.28.1
pandas==2.1.3
numpy==1.24.3
plotly==5.17.0
scikit-learn==1.3.2
scipy==1.11.4
seaborn==0.13.0
matplotlib==3.8.2
prince==0.7.1
streamlit-folium==0.25.0
folium==0.15.0
geopandas==0.14.1
```

---

## 9. CONCLUSIONES Y RECOMENDACIONES

### Hallazgos Principales
1. **Relaci√≥n significativa** entre nivel socioecon√≥mico y rendimiento acad√©mico
2. **Patrones geogr√°ficos** en la distribuci√≥n de resultados
3. **Factores predictivos** identificados mediante machine learning
4. **Perfiles de estudiantes** diferenciados por clustering

### Aplicaciones Pr√°cticas
- Dise√±o de pol√≠ticas educativas focalizadas
- Identificaci√≥n de poblaciones vulnerables
- Asignaci√≥n de recursos educativos
- Programas de apoyo acad√©mico

### Trabajo Futuro
- An√°lisis temporal de tendencias
- Incorporaci√≥n de variables adicionales
- Modelos predictivos m√°s complejos
- An√°lisis de redes sociales educativas

---

## 10. REFERENCIAS Y DOCUMENTACI√ìN

### Fuentes de Datos
- **ICFES**: Instituto Colombiano para la Evaluaci√≥n de la Educaci√≥n
- **DANE**: Departamento Administrativo Nacional de Estad√≠stica

### Metodolog√≠as Aplicadas
- Hair, J. F., et al. (2019). *Multivariate Data Analysis*
- Johnson, R. A., & Wichern, D. W. (2018). *Applied Multivariate Statistical Analysis*
- Hastie, T., et al. (2017). *The Elements of Statistical Learning*

### Herramientas Utilizadas
- **Streamlit Documentation**: https://docs.streamlit.io/
- **Plotly Documentation**: https://plotly.com/python/
- **Scikit-learn Documentation**: https://scikit-learn.org/

---

**Nota**: Este documento constituye la evidencia completa del proyecto de an√°lisis multivariante de las pruebas Saber Pro, incluyendo el c√≥digo fuente, metodolog√≠a aplicada y resultados obtenidos.
